---
title: "Advanced Customer Segmentation"
subtitle: "Using R + Python"
author: 
date: 
output: 
   html_document:
       theme: flatly
       toc: true
       toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo    = TRUE,
    message = FALSE,
    warning = FALSE
)
```

# References

This tutorial is based on the [Python Customer Segmentation Tutorial](https://www.kaggle.com/fabiendaniel/customer-segmentation/notebook) by F. Daniel. 

A few changes:
1. Implement order history into prediction (use invoice counts)
2. Use `R` for data wrangling, preprocessing, and visualization
3. Keep `Python` for the Machine Learning and Clustering

# Libraries

```{r}
# Time Series
library(lubridate)
library(timetk)

# Text
library(tidytext)

# Visualization
library(plotly)
library(ggwordcloud)
library(ggiraphExtra)

# Preprocessing
library(recipes)

# Core
library(tidyverse)
library(tidyquant)
```

# Setup R's Python interface (Conda py3.8 Environment)

## Preparation 



- Install the Anaconda Distribution
- Get Python Scikit Learn Setup in R
- Make "py3.8" conda environment with `scikit-learn`, `numpy`, `pandas` and `matplotlib`.

## Reticulate Setup

```{r}
library(reticulate)
```

```{r}
# Replace this with your conda environment containing sklearn, pandas, & numpy
use_condaenv("py3.8", required = TRUE)
```



# Data (R)

```{r}
ecommerce_raw_tbl <- read_csv("data/ecommerce_data.csv")
ecommerce_raw_tbl %>% glimpse()
```

## Format the Transactions Data

```{r}
ecommerce_tbl <- ecommerce_raw_tbl %>%
    mutate(InvoiceDate = mdy_hms(InvoiceDate)) %>%
    select(contains("ID"), contains("Invoice"), everything()) %>%
    mutate(CustomerID = as.character(CustomerID)) %>%
    filter(UnitPrice > 0) %>%
    mutate(PriceExt = Quantity * UnitPrice)

ecommerce_tbl %>% glimpse()
```

## Visualization Transactions by Time

```{r}
summarize_by <- "day"

ecommerce_tbl %>%
    summarise_by_time(InvoiceDate, .by = summarize_by, revenue = sum(PriceExt)) %>%
    plot_time_series(InvoiceDate, revenue, .smooth_period = "3 months")
```



# Part 1 - Product Features (R)


## Products Table

```{r}
products_raw_tbl <- ecommerce_raw_tbl %>%
    select(StockCode, Description, UnitPrice) %>%
    distinct() %>%
    arrange(desc(StockCode))

products_raw_tbl 
```

### Aggregation

Problems:

- Stock codes have varying descriptions
- Stock Codes have varying Prices

```{r}
products_raw_tbl %>% filter(StockCode == "21786")
```

Solution:
- Use most frequent Description (mode)
- Use median UnitPrice (median)

```{r}
mode <- function(x) {
    as.character(x) %>% 
        table() %>% 
        sort(decreasing = TRUE) %>% 
        names() %>% 
        .[1]
}

products_summarized_tbl <- products_raw_tbl %>%
    filter(!is.na(Description)) %>%
    group_by(StockCode) %>%
    summarise(
        n = n(),
        median_unit_price = median(UnitPrice),
        mode_description  = mode(Description)
    ) %>%
    ungroup() 

products_summarized_tbl
```

### Filter

#### Bad Stock Codes

Problem:

- Stock Codes that aren't products: DOT, M, POST, etc

```{r}
products_summarized_tbl %>%
    arrange(desc(n))
```

Solution:
- Remove "DOT", "M", "POST", "D", "S", "AMAZONFEE", "BANK CHARGES"

```{r}
products_summarized_tbl %>%
    filter(!StockCode %in% c("DOT", "M", "POST", "D", "S", "AMAZONFEE", "BANK CHARGES")) %>%
    arrange(desc(n))
```

#### Negative Median Prices

Last Problems:
- Some unit prices are zero
- Missed "CRUK" that doesn't describe a physical product

Solution:
- Remove them

```{r}
products_summarized_tbl %>%
    filter(!StockCode %in% c("DOT", "M", "POST", "D", "S", "AMAZONFEE", "BANK CHARGES")) %>%
    arrange(median_unit_price) %>%
    filter(median_unit_price > 0) 
```

#### Apply Filters

```{r}
products_filtered_tbl <- products_summarized_tbl %>%
    filter(!StockCode %in% c("DOT", "M", "POST", "D", "S", "AMAZONFEE", "BANK CHARGES", "CRUK")) %>%
    filter(median_unit_price > 0)

products_filtered_tbl
```





## Product Feature Engineering (R)

Problems:

- Dataset has no categories.
- Need categories to learn which customers are similar.

Solutions: 
- __Price Feature Engineering__ will help group products by price categories
- __Text Feature Engineering__ is essential to compare description fields


### Price Feature Engineering


```{r}
products_filtered_tbl %>%
    ggplot(aes(median_unit_price)) +
    geom_histogram()
```

```{r}
products_filtered_tbl %>%
    ggplot(aes(log(median_unit_price))) +
    geom_histogram()
```


### Text Feature Engineering

#### Create a Dictionary

- Uses `tidytext` to unnest tokens
- Stems the tokens using `hunspell` to return only the root of the word

```{r}
product_dictionary_raw_tbl <- products_filtered_tbl %>%
    select(StockCode, mode_description) %>%
    unnest_tokens(terms, mode_description, token = "words") %>%
    mutate(terms = hunspell::hunspell_stem(terms)) %>%
    unnest(terms) %>%
    mutate(n = 1)

product_dictionary_raw_tbl
```

#### Term Frequency

- Count Frequency
- Remove stop words (e.g. "of" has no meaning)
- Remove colors (e.g. "pink" has no meaning)
- Remove terms with numbers (e.g. "130.5cm" has no meaning)

```{r}
term_frequency_tbl <- product_dictionary_raw_tbl %>%
    # Remove unnecessary terms
    anti_join(stop_words, by = c("terms" = "word")) %>%
    filter(!terms %in% colors()) %>%
    filter(!terms %>% str_detect(pattern = "[0-9]")) %>%
    # Summarize
    group_by(terms) %>%
    summarise(
        n = sum(n)
    ) %>%
    arrange(desc(n)) 

term_frequency_tbl
```

#### Visualizations

##### Plotly

```{r, fig.height=8}
g <- term_frequency_tbl %>%
    slice(1:40) %>%
    mutate(terms = as_factor(terms) %>% fct_rev()) %>%
    ggplot(aes(n, terms)) +
    geom_point() +
    expand_limits(x = 0)

ggplotly(g)
```

##### Wordcloud

```{r}
term_frequency_tbl %>%
    slice(1:100) %>%
    mutate(terms = as_factor(terms) %>% fct_rev()) %>%
    ggplot() +
    geom_text_wordcloud_area(aes(label = terms, size = n, color = n)) +
    scale_size_area(max_size = 14) +
    scale_color_viridis_c(direction = -1) +
    theme(plot.background  = element_rect(fill = "black"), 
          panel.background = element_rect(fill = "black"))
```

#### Text Features


```{r}
top_100_terms <- term_frequency_tbl %>% slice(1:100) %>% pull(terms)

product_text_features_tbl <- product_dictionary_raw_tbl %>%
    filter(terms %in% top_100_terms) %>%
    pivot_wider(
        names_from  = terms, 
        values_from = n, 
        values_fill = list(n = 0), 
        values_fn   = list(n = sum)
    )

product_text_features_tbl
```

### Join Product Categorization Data

```{r}
products_joined_tbl <- products_filtered_tbl %>%
    left_join(product_text_features_tbl) %>%
    drop_na()

products_joined_tbl
```

### Prepare for Modeling

```{r, paged.print = FALSE}
recipe_spec_products <- recipe(~ ., data = products_joined_tbl) %>%
    step_rm(StockCode, n, mode_description) %>%
    step_log(median_unit_price, offset = 1) %>%
    step_range(median_unit_price) %>%
    prep()

recipe_spec_products
```

```{r}
X           <- recipe_spec_products %>% juice() %>% as.matrix()
stock_code  <- products_joined_tbl %>% pull(StockCode)

rownames(X) <- stock_code

X[1:10, 1:10]
```



# Part 2 - Product Segmentation (Python)

```{python}
import numpy as np
import pandas as pd
```



## t-SNE

```{python}
from sklearn.manifold import TSNE
```

```{python}
X_embedded = TSNE(n_components=2, random_state=123).fit_transform(r.X)

pd.DataFrame(X_embedded)
```




## DBSCAN Cluster Assignments

```{python}
from sklearn.cluster import DBSCAN
```

```{python}
db = DBSCAN(eps=0.5, min_samples=40).fit(r.X)
```

```{python}
cluster_assignments_db = db.labels_
```

```{python}
pd.Series(cluster_assignments_db).unique()
```

## K-Means Cluster Assignments

```{python}
from sklearn.cluster import KMeans
```

```{python}
km = KMeans(n_clusters = 5, random_state=123).fit(r.X)
```

```{python}
cluster_assignments_km = km.labels_
```

```{python}
pd.Series(cluster_assignments_km).unique()
```



## Visualization

```{r}
cluster_assignments <- py$cluster_assignments_km

product_clusters_tbl <- py$X_embedded %>%
    as_tibble() %>%
    mutate(cluster = as.factor(cluster_assignments)) %>%
    mutate(StockCode = stock_code) %>%
    left_join(
        products_joined_tbl %>% 
            select(StockCode, median_unit_price, mode_description)
    ) %>%
    select(StockCode, median_unit_price, mode_description, everything())

product_clusters_tbl
```

### Product Cluster Map

```{r, eval=FALSE}
g <- product_clusters_tbl %>%
    mutate(mode_description = str_remove_all(mode_description, "[^A-Za-z0-9 -]")) %>%
    mutate(tooltip = str_glue("Desc: {mode_description}
                              Price: {median_unit_price}")) %>%
    ggplot(aes(V1, V2, color = cluster)) +
    geom_point(aes(text = tooltip), alpha = 0.7) +
    scale_color_tq() +
    theme_tq()

ggplotly(g, tooltip = "text")

```


### Term Frequency By Cluster

```{r}
term_frequency_tbl
```


```{r}
term_freq_by_cluster_tbl <- product_clusters_tbl %>%
    
    unnest_tokens(terms, mode_description, token = "words") %>%
    mutate(terms = hunspell::hunspell_stem(terms)) %>%
    unnest(terms) %>%
    
    # Remove stop words
    anti_join(stop_words, by = c("terms" = "word")) %>%
    filter(!terms %in% colors()) %>%
    filter(!terms %>% str_detect(pattern = "[0-9]")) %>%
    # filter(!terms %>% str_detect("(set)|(heart)")) %>%
    
    mutate(n = 1) %>%
    
    # Group by cluster & select top N terms
    group_by(cluster, terms) %>%
    summarise(n = sum(n)) %>%
    arrange(desc(n), .by_group = TRUE) %>%
    slice(1:50) 

term_freq_by_cluster_tbl
```

### Plotly

```{r, fig.height=8}
g <- term_freq_by_cluster_tbl %>%
    slice(1:5) %>%
    ungroup() %>%
    arrange(desc(n)) %>%
    mutate(terms = as_factor(terms) %>% fct_rev()) %>%
    ggplot(aes(n, terms, color = cluster)) +
    geom_point() +
    expand_limits(x = 0) +
    facet_wrap(~ cluster, ncol = 2, scales = "free_y") +
    theme_tq() +
    scale_color_tq() +
    labs(y = "")

ggplotly(g)
```


### Wordcloud by Cluster

```{r}
term_freq_by_cluster_tbl %>%
    ggplot() +
    geom_text_wordcloud_area(aes(label = terms, size = n, color = cluster)) +
    # scale_size_area(max_size = 15) +
    scale_color_viridis_d(direction = -1) +
    facet_wrap(~ cluster, ncol = 2) +
    theme_minimal() +
    theme(plot.background  = element_rect(fill = "black"), 
          panel.background = element_rect(fill = "black")) 
```


# Part 3 - Customer Features (R)


## Join Product Categories 

```{r}
transaction_product_clusters_tbl <- ecommerce_tbl %>%
    filter(PriceExt > 0) %>%
    left_join(
        product_clusters_tbl %>% 
            group_by(StockCode) %>%
            slice(1) %>%
            ungroup()
    ) %>%
    select(
        CustomerID, InvoiceNo, InvoiceDate, StockCode, mode_description, cluster, PriceExt
    )

transaction_product_clusters_tbl
```

## Characterize Spend Habits

```{r}
customer_spend_habits_tbl <- transaction_product_clusters_tbl %>%
    
    # Get spend by Invoice (Purchase)
    group_by(CustomerID, InvoiceNo) %>%
    summarize(order_value = SUM(PriceExt)) %>%
    ungroup() %>%
    
    # Aggregate by Customer ID
    group_by(CustomerID) %>%
    summarize(
        count  = n(),
        min    = MIN(order_value),
        mean   = AVERAGE(order_value),
        max    = MAX(order_value),
        sum    = SUM(order_value)
    ) %>%
    ungroup()

customer_spend_habits_tbl
```

## Characterize Product Categories

```{r}
customer_product_habits_tbl <- transaction_product_clusters_tbl %>%
    select(CustomerID, PriceExt, cluster) %>%
    group_by(CustomerID, cluster) %>%
    summarize(PriceExt = SUM(PriceExt)) %>%
    mutate(prop = PriceExt / SUM(PriceExt)) %>%
    ungroup() %>%
    select(-PriceExt) %>%
    pivot_wider(names_from   = cluster, 
                values_from  = prop, 
                values_fill  = list(prop = 0), 
                names_prefix = "cat_") 

customer_product_habits_tbl
```

## Characterize Recency in Purchases

```{r}
max_date <- max(as_date(transaction_product_clusters_tbl$InvoiceDate))

customer_recency_habits_tbl <- transaction_product_clusters_tbl %>%
    count(CustomerID, InvoiceDate) %>%
    mutate(InvoiceDate = as_date(InvoiceDate)) %>%
    group_by(CustomerID) %>%
    summarise(
        FirstPurchase = -1 * (min(InvoiceDate) - max_date)  / ddays(1),
        LastPurchase  = -1 * (max(InvoiceDate) - max_date)  / ddays(1)
    )

customer_recency_habits_tbl
```

## Join Customer Habits

```{r}
customer_habits_joined_tbl <- customer_spend_habits_tbl %>%
    left_join(customer_product_habits_tbl) %>%
    left_join(customer_recency_habits_tbl)

customer_habits_joined_tbl
```



## Prepare for Modeling

```{r, paged.print = FALSE}
recipe_spec_customers <- recipe(~ ., data = customer_habits_joined_tbl) %>%
    step_rm(CustomerID) %>%
    step_log(all_predictors(), -contains("cat_"), offset = 1) %>%
    step_range(all_predictors(), -contains("cat_")) %>%
    prep()

recipe_spec_customers
```


```{r}
recipe_spec_customers %>% juice()
```

```{r}
X           <- recipe_spec_customers %>% juice() %>% as.matrix()
customer_id <- customer_habits_joined_tbl %>% pull(CustomerID)

rownames(X) <- customer_id

X[1:10, ]
```

# Part 4 - Customer Segmentation (Python)

```{python}
import numpy as np
import pandas as pd
```

## t-SNE

```{python}
from sklearn.manifold import TSNE
```

```{python}
X_embedded = TSNE(n_components=2, random_state=123).fit_transform(r.X)

pd.DataFrame(X_embedded)
```



## K-Means Cluster Assignments

```{python}
from sklearn.cluster import KMeans
```

```{python}
km = KMeans(n_clusters = 6, random_state=123).fit(r.X)
```

```{python}
cluster_assignments_km = km.labels_
```

```{python}
pd.Series(cluster_assignments_km).unique()
```

## Customers by Cluster

```{r}
cluster_assignments <- py$cluster_assignments_km

customer_clusters_tbl <- py$X_embedded %>%
    as_tibble() %>%
    mutate(cluster = as.factor(cluster_assignments)) %>%
    mutate(CustomerID = customer_id) %>%
    left_join(
        customer_habits_joined_tbl 
    ) %>%
    select(CustomerID, everything())

customer_clusters_tbl
```

## Customer Cluster Morphology

```{r}
cluster_morphology_tbl <- customer_clusters_tbl %>%
    group_by(cluster) %>%
    summarise_at(vars(count:LastPurchase), .funs = mean)

cluster_morphology_tbl
```


```{r}
cluster_morphology_ggplot <- cluster_morphology_tbl %>%
    select(cluster, count, sum, cat_0, cat_1, cat_2, cat_3, cat_4, cat_NA) %>%
    ggRadar(aes(color = cluster), size = 1) +
    facet_wrap(~ cluster) +
    theme_minimal() +
    scale_color_viridis_d(begin = 0.25) +
    scale_fill_viridis_d(begin = 0.25) +
    labs(title = "Customer Morphology") +
    theme(
        plot.background  = element_rect(fill = "black"), 
        panel.background = element_rect(fill = "black"), 
        legend.position  = "none", 
        text = element_text(color = "white"), 
        axis.text = element_text(color = "grey80")
    ) 

cluster_morphology_ggplot
```

## Prepare for Machine Learning

```{r, paged.print = FALSE}
data <- customer_clusters_tbl %>%
    select(cluster, count, mean, contains("cat"))

recipe_spec_customer_prediction <- recipe(cluster ~ ., data = data) %>%
    step_log(mean) %>%
    step_range(mean) %>%
    step_mutate(cluster = as.integer(as.character(cluster)), skip = TRUE) %>%
    prep()

recipe_spec_customer_prediction %>% juice()
```

```{r}
X <- recipe_spec_customer_prediction %>% 
    juice() %>% 
    select(-cluster) %>% 
    as.matrix()

Y <- recipe_spec_customer_prediction %>% 
    juice() %>%
    pull(cluster)

X[1:10, ]

Y[1:10]
```


# Part 5 - Customer Prediction

## Setup

```{python}
from sklearn import model_selection, metrics
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.svm import SVC
from sklearn import linear_model
```

## Train/Test

```{python}
X = r.X
Y = r.Y
```

```{python}
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 123)
```

```{python}
X_train.shape
```


## SVM Classifier

```{python}
svm_grid = GridSearchCV(
    estimator  = SVC(random_state=123, kernel='rbf'),
    param_grid = [{'C':np.logspace(-2, 2, 10)}],
    cv         = 5
)

svm_grid
```

```{python}
svm_fit = svm_grid.fit(X=X_train, y=Y_train)
```


```{python}
predictions_svm = svm_fit.predict(X_test)
predictions_svm[1:10]
```

```{python}
metrics.accuracy_score(Y_test, predictions_svm)
```




## Logistic Regression

```{python}
logistic_reg_grid = GridSearchCV(
    estimator  = linear_model.LogisticRegression(max_iter=4000),
    param_grid = [{'C':np.logspace(-2,2,20)}],
    cv         = 5
)

logistic_reg_grid
```

```{python}
logistic_reg_fit = logistic_reg_grid.fit(X=X_train, y=Y_train)
```


```{python}
predictions_logistic_reg = logistic_reg_fit.predict(X_test)
predictions_logistic_reg[1:10]
```


```{python}
metrics.accuracy_score(Y_test, predictions_logistic_reg)
```


# Save Model 


## Model 

```{python}
import joblib
```


```{python, eval = FALSE}
joblib.dump(logistic_reg_fit, "models/logistic_reg.sav")
```


```{python}
lr_model = joblib.load("models/logistic_reg.sav")
lr_model
```

```{python}
metrics.accuracy_score(Y_test, lr_model.predict(X_test))
```

## BONUS: Sourcing Python from R (Needed for Shiny)

```{r}
source_python("py/logistic_reg.py")
```

```{r}
X[2:5,]
```

```{r}
X[1:5,] %>% lr_predict()
```

# Save Preprocessing

## Product Clusters

```{r, eval=F}
product_clusters_tbl %>% write_rds("preprocessing/product_clusters_tbl.rds")
```


## Customer History

```{r, eval=F}
customer_habits_joined_tbl %>% write_rds("preprocessing/customer_habits_joined_tbl.rds")
```

## Preprocessing Recipe

```{r, eval=F}
recipe_spec_customer_prediction %>% write_rds("preprocessing/recipe_spec_customer_prediction.rds")
```

## Cluster Morphology 

```{r, eval=F}
cluster_morphology_tbl %>% write_rds("preprocessing/cluster_morphology_tbl.rds")
```


```{r, eval=F}
cluster_morphology_ggplot %>% write_rds("preprocessing/cluster_morphology_ggplot.rds")
```

